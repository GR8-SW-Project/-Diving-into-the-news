{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBS 뉴스 Crawling\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.sbs.co.kr\"\n",
    "url_main = \"https://news.sbs.co.kr/news/newsflash.do?pageDate=20210920\"\n",
    "url_target = \"https://news.sbs.co.kr/news/newsflash.do?pageDate=20210920\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['title', 'headline', 'date', 'news_link', 'content', 'category']\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----- Start -----')\n",
    "\n",
    "while True:\n",
    "    current_page = requests.get(url=url_main) # main page(idx=1)\n",
    "    while True:\n",
    "        if current_page.status_code == 200:\n",
    "            current_page = current_page.text\n",
    "            break\n",
    "        else:\n",
    "            print(current_page.status_code)\n",
    "            \n",
    "    # sleep(5)\n",
    "    soup = bs(current_page, 'html.parser')\n",
    "    \n",
    "    link = soup.find('link', {'rel' : 'canonical'})['href']\n",
    "    date = link.split('?pageDate=')[-1]\n",
    "    if date == '20220920':\n",
    "        break\n",
    "    \n",
    "    next_date = list(soup.find('ol',  {'class' : 'mdp_date_list'})) # ['\\n', date]\n",
    "    next_date = [e for e in next_date if e != '\\n'][1] # remove '\\n'\n",
    "    next_url = next_date.find('a')['href']\n",
    "    \n",
    "    idx_p = 1\n",
    "\n",
    "    print()\n",
    "    print('date: ' + date)\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        print()\n",
    "        print('pageIdx=' + str(idx_p))\n",
    "        print()\n",
    "        \n",
    "        current_page = requests.get(url=url_target) # next page(idx > 1)\n",
    "        while True:\n",
    "            if current_page.status_code == 200:\n",
    "                current_page = current_page.text\n",
    "                break\n",
    "            else:\n",
    "                print(current_page.status_code)\n",
    "            \n",
    "        # sleep(5)\n",
    "        soup = bs(current_page, 'html.parser')\n",
    "        \n",
    "        flag = soup.find('div', {'class' : 'w_news_list type_issue'}).text.strip() # content presence\n",
    "        if flag == '해당 일자의 뉴스 정보가 없습니다.':\n",
    "            break\n",
    "        \n",
    "        list_news = list(soup.find_all('li', {'itemtype' : 'http://schema.org/NewsArticle'})) # current page news\n",
    "        \n",
    "        for news in list_news:\n",
    "            try:\n",
    "                title = news.find('strong').text # title\n",
    "                headline = news.find('span', {'class' : 'read'}).text # headline\n",
    "                # thumb = news.find('img')['src'] # thumbnail\n",
    "                date = news.find('span', {'class' : 'date'}).text # date\n",
    "                link_news = news.find('meta')['itemid'] # news link\n",
    "            except (AttributeError, TypeError):\n",
    "                title = None\n",
    "                headline = None\n",
    "                # thumb = None\n",
    "                date = None\n",
    "                link_news = None\n",
    "            \n",
    "            article_page = requests.get(url=link_news) # news page\n",
    "            while True:\n",
    "                if article_page.status_code == 200:\n",
    "                    article_page = article_page.text\n",
    "                    break\n",
    "                else:\n",
    "                    print(article_page.status_code)\n",
    "                    \n",
    "            # sleep(5) \n",
    "            soup2 = bs(article_page, 'html.parser')\n",
    "            \n",
    "            try:\n",
    "                content = soup2.find('div', {'itemprop' : 'articleBody'}).text.replace('\\n', '') # content\n",
    "                category = soup2.find('li', {'class' : 'cate03'}).text # category\n",
    "            except (AttributeError, TypeError):\n",
    "                content = None\n",
    "                category = None\n",
    "\n",
    "            if title != None and headline != None and date != None and link_news != None and content != None and category != None:\n",
    "                data.append([title, headline, date, link_news, content, category])\n",
    "            \n",
    "            print(len(data))\n",
    "        \n",
    "        idx_p += 1\n",
    "        page_idx = '&pageIdx=' + str(idx_p)\n",
    "        url_target = url_main + page_idx\n",
    "            \n",
    "    url_main = url + next_url\n",
    "    url_target = url + next_url\n",
    "\n",
    "print()\n",
    "print('----- End -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data=data, columns=columns)\n",
    "dataset['site'] = 'sbs'\n",
    "dataset.to_csv('./Data/kbs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# span class read : 헤드라인 ~\n",
    "# span class thumb : 썸네일 ~\n",
    "# strong class sub : 뉴스 제목 ~\n",
    "# span class data : 뉴스 날짜 및 시간 ~ \n",
    "# meta itemid : 뉴스 링크 ~\n",
    "\n",
    "\n",
    "# li class cate03 : 카테고리 ~\n",
    "# div class itemprop articleBody : 뉴스 내용\n",
    "\n",
    "# div class mdp_inner -> a href #page : \n",
    "# div class w_news_list type_issue : 뉴스 정보"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DINS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e590ccd7b691ab80e99d866104013fd94fd750e462d26c4ca346d9f79ec019e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
