{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KBS 뉴스 Crawling\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# from selenium import webdriver as wd\n",
    "# from selenium.webdriver.chrome.service impaort Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "# import subprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.kbs.co.kr\"\n",
    "url_main = \"https://news.kbs.co.kr/news/list.do?ref=pSiteMap#20220921&1\"\n",
    "url_target = \"https://news.kbs.co.kr/news/list.do?ref=pSiteMap#20220921&1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['title', 'headline', 'date', 'news_link', 'content', 'category']\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "currnet_page = requests.get(url=url_main).text\n",
    "soup = bs(currnet_page, 'html.parser')\n",
    "# content = soup.find('ul', {'class' : 'list-type list-thumb'})\n",
    "article = soup.find_all('li', {'style' : 'thumbnailNewsList'})\n",
    "\n",
    "print(article)\n",
    "# content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Start -----\n",
      "\n",
      "page: 1\n",
      "\n",
      "\n",
      "pageIdx=1\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "\n",
      "pageIdx=2\n",
      "\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "\n",
      "pageIdx=3\n",
      "\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "\n",
      "pageIdx=4\n",
      "\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "\n",
      "pageIdx=5\n",
      "\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "\n",
      "pageIdx=6\n",
      "\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "\n",
      "pageIdx=7\n",
      "\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "print('----- Start -----')\n",
    "\n",
    "n_page = 0\n",
    "\n",
    "while True:\n",
    "    current_page = requests.get(url=url_main) # main page(idx=1)\n",
    "    while True:\n",
    "        if current_page.status_code == 200:\n",
    "            current_page = current_page.text\n",
    "            break\n",
    "        else:\n",
    "            print(current_page.status_code)\n",
    "        \n",
    "    soup = bs(current_page, 'html.parser')\n",
    "    sleep(0.5)\n",
    "    \n",
    "    link = soup.find('link', {'rel' : 'canonical'})['href']\n",
    "    date = link.split('?pageDate=')[-1]\n",
    "    if date == '20220920':\n",
    "        break\n",
    "    \n",
    "    next_date = list(soup.find('ol',  {'class' : 'mdp_date_list'})) # ['\\n', date]\n",
    "    next_date = [e for e in next_date if e != '\\n'][1] # remove '\\n'\n",
    "    next_url = next_date.find('a')['href']\n",
    "    \n",
    "    idx_p = 1\n",
    "    \n",
    "    n_page += 1\n",
    "    print()\n",
    "    print('page: ' + str(n_page))\n",
    "    print()\n",
    "    \n",
    "    while True:\n",
    "        print()\n",
    "        print('pageIdx=' + str(idx_p))\n",
    "        print()\n",
    "        \n",
    "        current_page = requests.get(url=url_target) # next page(idx > 1)\n",
    "        while True:\n",
    "            if current_page.status_code == 200:\n",
    "                current_page = current_page.text\n",
    "                break\n",
    "            else:\n",
    "                print(current_page.status_code)\n",
    "            \n",
    "        soup = bs(current_page, 'html.parser')\n",
    "        sleep(5)\n",
    "        \n",
    "        flag = soup.find('div', {'class' : 'w_news_list type_issue'}).text.strip() # content presence\n",
    "        if flag == '해당 일자의 뉴스 정보가 없습니다.':\n",
    "            break\n",
    "        \n",
    "        list_news = list(soup.find_all('li', {'itemtype' : 'http://schema.org/NewsArticle'})) # current page news\n",
    "        \n",
    "        for news in list_news:\n",
    "            try:\n",
    "                title = news.find('strong').text # title\n",
    "                headline = news.find('span', {'class' : 'read'}).text # headline\n",
    "                # thumb = news.find('img')['src'] # thumbnail\n",
    "                date = news.find('span', {'class' : 'date'}).text # date\n",
    "                link_news = news.find('meta')['itemid'] # news link\n",
    "            except AttributeError as e:\n",
    "                title = None\n",
    "                headline = None\n",
    "                date = None\n",
    "                link_news = None\n",
    "            \n",
    "            article_page = requests.get(url=link_news) # news page\n",
    "            while True:\n",
    "                if article_page.status_code == 200:\n",
    "                    article_page = article_page.text\n",
    "                    break\n",
    "                else:\n",
    "                    print(current_page.status_code)\n",
    "                \n",
    "            soup2 = bs(article_page, 'html.parser')\n",
    "            sleep(5)\n",
    "            \n",
    "            try:\n",
    "                content = soup2.find('div', {'itemprop' : 'articleBody'}).text.replace('\\n', '') # content\n",
    "                category = soup2.find('li', {'class' : 'cate03'}).text # category\n",
    "            except AttributeError as e:\n",
    "                content = None\n",
    "                category = None\n",
    "\n",
    "            if title != None and headline != None and date != None and link_news != None and content != None and category != None:\n",
    "                data.append([title, headline, date, link_news, content, category])\n",
    "            \n",
    "            print(len(data))\n",
    "        \n",
    "        idx_p += 1\n",
    "        page_idx = '&pageIdx=' + str(idx_p)\n",
    "        url_target = url_main + page_idx\n",
    "            \n",
    "    url_main = url + next_url\n",
    "\n",
    "print()\n",
    "print('----- End -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20170101'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = requests.get(url=url_start).text\n",
    "# soup = bs(res, 'html.parser')\n",
    "# link = soup.find('link', {'rel' : 'canonical'})['href']\n",
    "# date = link.split('?pageDate=')[-1]\n",
    "# date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data=data, columns=columns)\n",
    "dataset.to_csv('./kbs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# span class read : 헤드라인 ~\n",
    "# span class thumb : 썸네일 ~\n",
    "# strong class sub : 뉴스 제목 ~\n",
    "# span class data : 뉴스 날짜 및 시간 ~ \n",
    "# meta itemid : 뉴스 링크 ~\n",
    "\n",
    "\n",
    "# li class cate03 : 카테고리 ~\n",
    "# div class itemprop articleBody : 뉴스 내용\n",
    "\n",
    "# div class mdp_inner -> a href #page : \n",
    "# div class w_news_list type_issue : 뉴스 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "current_page = requests.get(url = url_main)\n",
    "print(current_page.status_code)\n",
    "# soup = bs(current_page, 'html.parser')\n",
    "\n",
    "\n",
    "# list_news = list(soup.find_all('li', {'itemtype' : 'http://schema.org/NewsArticle'}))\n",
    "\n",
    "# link_news = list_news[0].find('img')['src'] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DINS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e590ccd7b691ab80e99d866104013fd94fd750e462d26c4ca346d9f79ec019e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
